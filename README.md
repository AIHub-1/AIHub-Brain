# AIHub-BCI
![header](https://capsule-render.vercel.app/api?type=waving&color=auto&height=300&section=header&text=AI%20Hub&fontSize=90&animation=fadeIn&fontAlignY=38&desc=%20ìš°ì£¼%20ìµœê°•%20BTS%20íŒ€&descAlignY=51&descAlign=62)
<p align='center'> Decorate GitHub Profile or any Repo like me! </p>
<p align='center'>
  <a href="https://github.com/kyechan99/capsule-render/labels/Idea">
    <img src="https://img.shields.io/badge/IDEA%20ISSUE%20-%23F7DF1E.svg?&style=for-the-badge&&logoColor=white"/>
  </a>
  <a href="#demo">
    <img src="https://img.shields.io/badge/DEMO%20-%234FC08D.svg?&style=for-the-badge&&logoColor=white"/>
  </a>
</p>


<div align=left>
	<h1>ğŸš€ AIHub ğŸš€</h1>
		<b>aaa</b>
</div>


<div align=left>
	<h1>ğŸ§  Starlab ğŸ§ </h1>
		<b>The open software package is designed for developing Brain-Computer Interfaces (BCIs) with various advanced pattern recognition algorithms.</b>
		<p>It includes detailed information and example codes for Motor Imagination (MI), Event-Related Potential (ERP), and Steady-State Visually Evoked Potential (SSVEP) in the 'Examples' folder.</p>
		<p>The package also features 'BMI_modules' with implementation functions, 'GUI module' for Graphic User Interface functions, and 'Paradigm' functions using Psychtoolbox.</p>
		<p>Additionally, it contains codes from other BCI groups and an OpenBMI demo. For questions or more information, visit http://openbmi.org.</p>
</div>

<div align=left>
<h1>ğŸ—£ï¸ NeuroTalk ğŸ—£ï¸</h1>
	<b>Voice Reconstruction from Brain Signals</b>
		<p>This repository is home to the official implementation of our novel algorithm aimed at reconstructing voice from EEG during imagined speech.</p>
	<b>Key Contributions</b>
		<p>- We've developed a generative model capable of extracting frequency characteristics and sequential information from neural signals to generate speech.</p>
		<p>- Addressed the constraint of imagined speech-based BTS system lacking ground truth voice by employing a domain adaptation method.</p>
		<p>- Demonstrated the potential of robust speech generation by training only several words or phrases, with the model showing capability to learn phoneme level information from brain signals.</p>
		<p>Please cite our work as per the provided citation if you find it useful. Contributions are welcome!</p>
</div>

<div align=left>
	<h1>ğŸ§  GigaScience ğŸ§ </h1>
		<b>aaa</b>
			<p>aaa</p>
</div>

<div align=left>
	<h1>ğŸ§  Diff-E ğŸ§ </h1>
		<b>Diff-E: EEG Imagined Speech Decoding Using Diffusion-based Learning </b>
		<b>Introduction</b>
			<p>Decoding EEG signals for imagined speech has been a complex task, primarily due to the high-dimensional nature of the data and a low signal-to-noise ratio.</p>
		<b>Solution</b>
			<p>Our study introduces Diff-E, a novel method that utilizes denoising diffusion probabilistic models (DDPMs) and a conditional autoencoder to address these challenges.</p>
		<b>Results</b>
			<p>We've found that Diff-E substantially outperforms traditional machine learning techniques and baseline models in terms of decoding accuracy.</p> 
		<b>Implications</b>
			<p>These findings indicate the potential effectiveness of DDPMs for EEG signal decoding, suggesting possible applications in the development of brain-computer interfaces that enable communication 				through imagined speech.</p> 
		<b>Status</b>
		<p>This work is currently under review for presentation at Interspeech 2023.</p>
</div>

<div align=left>
	<h1>ğŸ§  TNNLS ğŸ§ </h1>
		<b>aaa</b>
			<p>aaa</p>
</div>

<div align=left>
	<h1>ğŸ§  Sementics ğŸ§ </h1>
		<b>aaa</b>
			<p>aaa</p>
</div>

<div align=left>
<h1>ğŸ§  OnlineDemo ğŸ§ </h1>
<b>aaa</b>
<p>aaa<p>
</div>
