# AIHub-BCI

<div align=center>
	<img src="https://capsule-render.vercel.app/api?type=waving&color=auto&height=200&section=header&text=AI%20Hub!&fontSize=90" />	
<br>
</div>
</div>
<div align=center>
	<p>ðŸ§  AIHub ðŸ§ </p>
</div>
</div>
<div align=center>
---
</div>
<br>
<div align=center>
	<p>ðŸ§  OpenBMI ðŸ§ </p>
</div>
</div>
<div align=center>
The open software package is designed for developing Brain-Computer Interfaces (BCIs) with various advanced pattern recognition algorithms. 
</div>
<br>
</div>
<div align=center>
It includes detailed information and example codes for Motor Imagination (MI), Event-Related Potential (ERP), and Steady-State Visually Evoked Potential (SSVEP) in the 'Examples' folder. 
</div>
<br>
</div>
<div align=center>
The package also features 'BMI_modules' with implementation functions, 'GUI module' for Graphic User Interface functions, and 'Paradigm' functions using Psychtoolbox. 
</div>
<br>
</div>
<div align=center>
Additionally, it contains codes from other BCI groups and an OpenBMI demo. For questions or more information, visit http://openbmi.org.
</div>
<br>
<div align=center>
	<p>ðŸ§  NeuroTalk ðŸ§ </p>
</div>
</div>
<div align=center>
NeuroTalk: Voice Reconstruction from Brain Signals
This repository is home to the official implementation of our novel algorithm aimed at reconstructing voice from EEG during imagined speech.

Setup
Ensure Python 3.8 is installed and run pip install -r requirements.txt to install the required dependencies.

Training and Evaluation
Our model is trained and evaluated on both spoken and imagined EEG data. Pretrained models can be utilized for training and evaluation. Use the provided commands to run these operations.

Demo and Pretrained Models
Visit our Demo page and access our pretrained model trained on participant 1.

Key Contributions
We've developed a generative model capable of extracting frequency characteristics and sequential information from neural signals to generate speech.
Addressed the constraint of imagined speech-based BTS system lacking ground truth voice by employing a domain adaptation method.
Demonstrated the potential of robust speech generation by training only several words or phrases, with the model showing capability to learn phoneme level information from brain signals.
Please cite our work as per the provided citation if you find it useful. Contributions are welcome!
</div>
<div align=center>
	<p>ðŸ§  GigaScience ðŸ§ </p>
</div>
</div>
<div align=center>
	---
</div>
<br>
<div align=center>
	<p>ðŸ§  Diff-E-main ðŸ§ </p>
</div>
</div>
<div align=center>
	---
</div>
<br>
<div align=center>
	<p>ðŸ§  TNNLS ðŸ§ </p>
</div>
</div>
<div align=center>
	---
</div>
<br>
<div align=center>
	<p>ðŸ§  Sementics ðŸ§ </p>
</div>
</div>
<div align=center>
	---
</div>
<br>
<div align=center>
	<p>ðŸ§  OnlineDemo ðŸ§ </p>
</div>
</div>
<div align=center>
	---
</div>
