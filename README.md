![header](https://capsule-render.vercel.app/api?type=waving&color=auto&height=300&section=header&text=AIHub&fontColor=3C3C1F&fontSize=90&animation=fadeIn&fontAlignY=38&desc=%20AI%20Innovation%20Hub%20BCI%20Project&descAlignY=51&descAlign=62)

<div align=left>
<h1>🗣️ NeuroTalk 🗣️</h1>
	<b>Voice Reconstruction from Brain Signals</b>
		<p>The algorithm aimed at reconstructing voice from EEG during imagined speech.</p>
	<b>Key Contributions</b>
		<ul>
			<li>A generative model capable of extracting frequency characteristics and sequential information from neural signals to generate speech.</li>
			<li>Addressed the constraint of imagined speech-based BTS system lacking ground truth voice by employing a domain adaptation method.</li>
			<li>Demonstrated the potential of robust speech generation by training only several words or phrases, with the model showing the capability to learn phoneme level information from brain signals.</li>
			<li>This work is currently accepted for presentation at <a href="https://aaai-23.aaai.org/">AAAI 2023</a>.</li>
			<li>This work is based on <a href="https://github.com/youngeun1209/NeuroTalk">Neurotalk</a>. We will continue to develop and extend this foundational work..</li>
		</ul>
</div>

<div align=left>
	<h1>🏎️ Diff-E 🏎️</h1>
		<b>EEG Imagined Speech Decoding Using Diffusion-based Learning </b>
			<p>Decoding EEG signals for imagined speech has been a complex task, primarily due to the high-dimensional nature of the data and a low signal-to-noise ratio.</p>
		<b>Key Contributions</b>
			<ul>
				<li>Our study introduces Diff-E, a novel method that utilizes denoising diffusion probabilistic models (DDPMs) and a conditional 						autoencoder to address these challenges.</li>
				<li>We've found that Diff-E substantially outperforms traditional machine learning techniques and baseline models in terms of decoding 					accuracy.</li> 
				<li>These findings indicate the potential effectiveness of DDPMs for EEG signal decoding, suggesting possible applications in the 						development of brain-computer interfaces that enable communication through imagined speech.</li>
				<li>This work is currently accepted for presentation at <a href="https://www.interspeech2023.org/">Interspeech 2023</a>.</li>
				<li>This work is based on <a href="https://github.com/diffe2023/Diff-E">Diff-E</a>. We will continue to develop and extend this foundational work.</li>
			</ul>
</div>
				
<div align=left>
	<h1>❔ Semantics ❔</h1>
		<b>This folder contains code for the topic 'Reconstructing Sentences from Brain Signals using Contextual and Semantic Information'. It will be updated in the near future.</b>
			<ul>
			</ul>
</div>

<div align=left>
	<h1>💻 Online Demo 💻</h1>
		<b>This OnlineDemo folder will continue to be updated for an online demo system that is currently under development.</b>
			<ul></ul>
</div>

<br>

<div align=left>
	<h2> Open Source </h2>
		<ul>A comprehensive collection of Brain-Computer Interfaces (BCIs) and EEG signal analysis codes</ul>
		<ul>Focusing on EEG data analysis, BCIs development, and motor imagination analysis respectively.</ul>
		<ul>The folders offer varied functionalities like Motor Imagery, Steady-State Visual Evoked Potential, Event-Related Potential analysis, GUI module, and Paradigm functions.</ul>	
		<ul>For further information or inquiries, visit (http://openbmi.org).</ul>
</div>

<hr>
<div align=center>
	<h3>😎Collaborator!😎</h3>
		<p>
			<a href="https://github.com/SeoHyunLee-KU" target="_blank"><img src="https://img.shields.io/badge/S.-H. Lee-41454A?style=plastic&logo=aerlingus&logoColor=FF6347"/></a>
			<a href="https://github.com/yorgoon" target="_blank"><img src="https://img.shields.io/badge/S. Kim-41454A?style=plastic&logo=aerlingus&logoColor=4169E1"/></a>
			<a href="https://github.com/youngeun1209" target="_blank"><img src="https://img.shields.io/badge/Y.-E. Lee-41454A?style=plastic&logo=aerlingus&logoColor=3CB371"/></a>
			<a href="https://github.com/ByungKwanKo" target="_blank"><img src="https://img.shields.io/badge/B.-K. Ko-41454A?style=plastic&logo=aerlingus&logoColor=FFD700"/></a>
		</p>
		<p>
			<a href="https://github.com/jiwonLee-KU" target="_blank"><img src="https://img.shields.io/badge/J.-W. Lee-41454A?style=plastic&logo=aerlingus&logoColor=9370DB"/></a>
			<a href="https://github.com/2jungsun" target="_blank"><img src="https://img.shields.io/badge/J.-S. Lee-41454A?style=plastic&logo=aerlingus&logoColor=FF7F50"/></a>
			<a href="https://github.com/rlawnsdud99" target="_blank"><img src="https://img.shields.io/badge/J.-Y. Kim-41454A?style=plastic&logo=aerlingus&logoColor=87CEFA"/></a>
			<a href="https://github.com/park-jiha" target="_blank"><img src="https://img.shields.io/badge/J.-H. Park-41454A?style=plastic&logo=aerlingus&logoColor=32CD32"/></a>
			<a href="https://github.com/deokseonKim" target="_blank"><img src="https://img.shields.io/badge/D.-S. Kim-41454A?style=plastic&logo=aerlingus&logoColor=F08080"/></a>
		</p>
</div>

<div align=center>
	<h3>🚀Contact!🚀</h3>
		<p>
			<a href="mailto:j_y_kim@korea.ac.kr" target="_blank"><img src="https://img.shields.io/badge/j_y_kim@korea.ac.kr-41454A?style=plastic&logo=naver&logoColor=#03C75A"/></a>
		</p>
</div>

