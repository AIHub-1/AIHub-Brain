# AIHub-BCI
![header](https://capsule-render.vercel.app/api?type=waving&color=auto&height=300&section=header&text=AI%20Hub&fontColor=3C3C1F&fontSize=90&animation=fadeIn&fontAlignY=38&desc=%20Ïö∞Ï£º%20ÏµúÍ∞ï%20BTS%20ÌåÄ&descAlignY=51&descAlign=62)

<div align=left>
	<b>AI Hub Í∞ÑÎûµ ÏÜåÍ∞ú</b>
</div>


<div align=left>
	<h1>üß† Starlab üß†</h1>
		<b>The open software package, designed for developing Brain-Computer Interfaces (BCIs) with various advanced pattern recognition algorithms.</b>
		<ul>
			<li>Example codes for Motor Imagination (MI), Event-Related Potential (ERP), and Steady-State Visually Evoked 				Potential (SSVEP) in the 'Examples' folder.</li>
			<li>The package also features 'BMI_modules' with implementation functions, 'GUI module' for Graphic User Interface functions, and 'Paradigm' 					functions using Psychtoolbox.</li>
			<li>Additionally, it contains codes from other BCI groups and an OpenBMI demo. For questions or more information, visit http://openbmi.org.</li>
		</ul>
</div>

<div align=left>
<h1>üó£Ô∏è NeuroTalk üó£Ô∏è</h1>
	<b>Voice Reconstruction from Brain Signals</b>
		<p>The algorithm aimed at reconstructing voice from EEG during imagined speech.</p>
	<b>Key Contributions</b>
		<ul>
			<li>A generative model capable of extracting frequency characteristics and sequential information from neural signals to generate 				speech.</li>
			<li>Addressed the constraint of imagined speech-based BTS system lacking ground truth voice by employing a domain adaptation method.</li>
			<li>Demonstrated the potential of robust speech generation by training only several words or phrases, with the model showing capability to learn 				phoneme level information from brain signals.</li>
			<li>This work is currently under review for presentation at AAAI 2023.</li>
		</ul>
</div>

<div align=left>
	<h1>üß† GigaScience üß†</h1>
		<b>Comprehensive collection of EEG signal preprocess/analysis codes.</b>
			<ul>
				<li>It primarily focuses on the analysis of EEG data.</li>
				<li>This folder features different versions for Motor Imagery (MI), Steady-State Visual Evoked Potential (SSVEP), and Event-Related Potential (ERP).</li>
				<li>It serves as an invaluable resource for neuroscience researchers and data scientists.</li>
				<li>This content is particularly useful for those interested in EEG data processing and brain-computer interfaces.</li>
			</ul>
</div>

<div align=left>
	<h1>üß† Diff-E üß†</h1>
		<b>EEG Imagined Speech Decoding Using Diffusion-based Learning </b>
			<p>Decoding EEG signals for imagined speech has been a complex task, primarily due to the high-dimensional nature of the data and a low signal-to-noise ratio.</p>
		<b>Key Contributions</b>
			<ul>
				<li>Our study introduces Diff-E, a novel method that utilizes denoising diffusion probabilistic models (DDPMs) and a conditional 						autoencoder to address these challenges.</li>
				<li>We've found that Diff-E substantially outperforms traditional machine learning techniques and baseline models in terms of decoding 					accuracy.</li> 
				<li>These findings indicate the potential effectiveness of DDPMs for EEG signal decoding, suggesting possible applications in the 						development of brain-computer interfaces that enable communication through imagined speech.</li>
				<li>This work is currently under review for presentation at Interspeech 2023.</li>
			</ul>
</div>

<div align=left>
	<h1>üß† TNNLS üß†</h1>
		<b>Motor imagination analysis codes at openbmi.org.</b>
			<ul>
				<li>This folder provides comprehensive analysis codes for motor imagination, including pre-processing, feature extraction, 							classification, and evaluation modules.</li>
				<li>It includes codes for a basic motor imagination paradigm and example codes for setting up experiments and conducting analysis.</li>
				<li>For inquiries, refer to the website http://openbmi.org.</li>
			<ul>
</div>
				
<!--
<div align=left>
	<h1>üß† Sementics üß†</h1>
		<b>aaa</b>
			<p>aaa</p>
</div>

<div align=left>
	<h1>üß† OnlineDemo üß†</h1>
		<b>aaa</b>
			<p>aaa<p>
</div>
-->
